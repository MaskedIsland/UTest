{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"ultimate.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"code","metadata":{"id":"H0yuMa58fYGE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618367804027,"user_tz":-480,"elapsed":1726,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}},"outputId":"1f7c8dfd-7229-4d04-e9dc-5353e40968c4"},"source":["# 连接Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TvzEx6RQBeQd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618367805990,"user_tz":-480,"elapsed":3679,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}},"outputId":"21e2ed16-b541-4e53-b9d1-63cd2000a3a6"},"source":["import math, copy, time\n","import torch\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.sparse as sparse\n","from torch import linalg as LA\n","from sklearn.preprocessing import normalize\n","import numpy as np\n","import pandas as pd\n","import os\n","import sys\n","from tqdm import tqdm\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib\n","from scipy.sparse import coo_matrix\n","# %matplotlib inline\n","\n","# Device and random seed settings.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","SEED = 996\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if device==\"cuda\":\n","  torch.cuda.manual_seed(SEED)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["cpu\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b2ZJ7gphBeQj","executionInfo":{"status":"ok","timestamp":1618367805991,"user_tz":-480,"elapsed":3675,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["# # hyper-parameters\n","# LR = 1e-3\n","# NODE_FEATURES = 10\n","# EMB_DIM = 128\n","# GCN_HIDDEN = 16\n","# GCN_DROP = 0\n","# WEIGHT_DECAY = 5e-4\n","EPOCHS = 100\n","# ngf = 16 # 这个再调吧 \n","n_blocks = 3\n","side_len = 32 # 不可以动，接近算力极限\n","seq_len = 24 # 不建议动，要与数据一致\n","batch_size = 4\n","testing_set_rate = 0.3\n","heads = 8\n","d_model = 64\n","ngf = d_model // 4\n","\n","drive_prefix = \"/content/drive/My Drive/\"\n","corr_path_prefix = \"/content/drive/My Drive/UrbanTrans/fine-gain-data/c_\"\n","speed_path_prefix = \"/content/drive/My Drive/UrbanTrans/fine-gain-data/s_\"\n","nodes_features_path = \"/content/drive/My Drive/UrbanTrans/fine-gain-data/nodes_features.npy\"\n","model_out_path = drive_prefix + \"UrbanTrans/Model/ours_\"\n","data_date = 20121100 "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"EDClfSvCbm5c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618367836902,"user_tz":-480,"elapsed":34581,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}},"outputId":"93d0b5da-77e5-490e-bced-4cb87d06c4c0"},"source":["# Data loading...\n","\n","print(\"Data loading...\")\n","\n","corr_mat_seqs = []\n","speed_mat_seqs = []\n","for i in tqdm(range(1, 31)):\n","  corr_path = corr_path_prefix + str(data_date + i) + '.npy'\n","  speed_path = speed_path_prefix + str(data_date + i) + '.npy'\n","  corr_mat_seqs.append(np.load(corr_path))\n","  speed_mat_seqs.append(np.load(speed_path))\n","\n","\n","corr_mat_seqs = np.concatenate(corr_mat_seqs)\n","speed_mat_seqs = np.concatenate(speed_mat_seqs)\n","\n","\n","# 坏掉的全0的数据\n","# corr_mat_seqs = np.delete(corr_mat_seqs, 575, 0)\n","# speed_mat_seqs = np.delete(speed_mat_seqs, 575, 0)\n","\n","# 规范化阈值\n","speed_mat_seqs[speed_mat_seqs>70] = 70\n","speed_mat_seqs[speed_mat_seqs==0] = 70\n","\n","nodes_features = np.load(nodes_features_path)\n","\n","print(\"corr shape:\", corr_mat_seqs.shape, \"speed shape:\", speed_mat_seqs.shape, \\\n","      \"nodes features shape:\", nodes_features.shape)\n","print(\"corr size:\", corr_mat_seqs.nbytes, \"speed size:\", speed_mat_seqs.nbytes, \\\n","      \"nodes features size:\", nodes_features.nbytes)\n","\n","# 归一化\n","nodes_features = normalize(nodes_features, axis=0, norm='max')\n","corr_mat_seqs = corr_mat_seqs / corr_mat_seqs.max()\n","speed_mat_seqs = speed_mat_seqs / speed_mat_seqs.max()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/30 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Data loading...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 30/30 [00:24<00:00,  1.22it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["corr shape: (719, 1024, 1024) speed shape: (719, 32, 32) nodes features shape: (1024, 148)\n","corr size: 6031409152 speed size: 5890048 nodes features size: 1212416\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UU_yzptUS7wG","executionInfo":{"status":"ok","timestamp":1618367836904,"user_tz":-480,"elapsed":34578,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["class UrbanDataset(Dataset):\n","  \"\"\"Urban dataset\"\"\"\n","\n","  def __init__(self, corr_mat, speed_mat):\n","    \"\"\"\n","    Construct a model from hyperparameters.\n","    \n","    Parameters:\n","     corr_mat - (np.array) (seq_len, side_len**2, side_len**2)\n","     speed_mat - (np.array) (seq_len, side_len, side_len)\n","     \n","    Returns:\n","     Urban dataset.\n"," \n","    Raises:\n","     None, todo\n","    \"\"\"\n","    self.corr_mat = torch.from_numpy(corr_mat)\n","    self.speed_mat = torch.from_numpy(speed_mat)\n","\n","  def __len__(self):\n","    return len(self.corr_mat) - 24\n","\n","  def __getitem__(self, idx):\n","    corr_seq = self.corr_mat[idx : idx+24]\n","    speed_seq = self.speed_mat[idx : idx+24]\n","\n","    return corr_seq, speed_seq  \n","\n","class UrbanSparseDataset(Dataset):\n","  \"\"\"\n","  Urban sparse dataset\n","  \"\"\"\n","\n","training_set_size = int(len(corr_mat_seqs) * (1 - testing_set_rate))\n","testing_set_size = len(corr_mat_seqs) - training_set_size\n","\n","urban_training_set = UrbanDataset(corr_mat_seqs[:training_set_size], speed_mat_seqs[:training_set_size])\n","urban_testing_set = UrbanDataset(corr_mat_seqs[training_set_size:], speed_mat_seqs[training_set_size:])\n","train_dataloader = DataLoader(urban_training_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n","test_dataloader = DataLoader(urban_testing_set, batch_size=1, shuffle=False, num_workers=0, drop_last=True)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"kPk_g6mKBeQj","executionInfo":{"status":"ok","timestamp":1618367836905,"user_tz":-480,"elapsed":34576,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["def clones(module, N):\n","  \"Produce N identical layers.\"\n","  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n","\n","def subsequent_mask(size):\n","  \"Mask out subsequent positions.\"\n","  attn_shape = (1, size, size)\n","  subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","  return torch.from_numpy(subsequent_mask) == 0\n","\n","# Hinton的论文\n","class LayerNorm(nn.Module):\n","  \"Construct a layernorm module (See citation for details).\"\n","  def __init__(self, features, eps=1e-6):\n","    super(LayerNorm, self).__init__()\n","    self.a_2 = nn.Parameter(torch.ones(features))\n","    self.b_2 = nn.Parameter(torch.zeros(features))\n","    self.eps = eps\n","\n","  def forward(self, x):\n","    mean = x.mean(-1, keepdim=True)\n","    std = x.std(-1, keepdim=True)\n","    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkJDmqKyqlVd","executionInfo":{"status":"ok","timestamp":1618367836905,"user_tz":-480,"elapsed":34571,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["# GAT for weights shared batch training\n","class GATConv(nn.Module):\n","  \"\"\"\n","  Simple PyTorch Implementation of the Graph Attention layer.\n","  \"\"\"\n","  # https://dsgiitr.com/blogs/gat/\n","\n","  def __init__(self, in_features, out_features, heads, alpha=0.2, dropout=0.6 ,batch_size=4, seq_len=24, concat=True):\n","    super(GATConv, self).__init__()\n","    self.in_features = in_features     \n","    self.out_features = out_features     \n","    self.concat = concat # conacat = True for all layers except the output layer.\n","    self.heads = heads \n","    self.batch_size = batch_size\n","    self.seq_len = seq_len\n","\n","    # Xavier Initialization of Weights\n","    # Alternatively use weights_init to apply weights of choice \n","    self.W = nn.Parameter(torch.Tensor(in_features, heads * out_features))\n","    # nn.init.xavier_uniform_(self.W.data, gain=1.414)\n","    self.a = nn.ParameterList([nn.Parameter(torch.Tensor(2 * out_features, 1)) for _ in range(heads)])\n","    # nn.init.xavier_uniform_(self.a.data, gain=1.414)\n","\n","    # LeakyReLU\n","    self.leakyrelu = nn.LeakyReLU(alpha)\n","    self.softmax =  nn.Softmax(dim=-1)\n","    self.elu = nn.ELU()\n","    self.dropout = nn.Dropout(dropout) # drop prob = 0.6\n","\n","\n","  def forward(self, input, adj):\n","    print(\"input size:\", input.size(), \"adj size:\", adj.size(), \"W size\", self.W.size())\n","    # (nodes_num, feature_dim), (batch_size, seq_len, side_len**2, side_len**2)\n","    # Linear Transformation\n","    N = input.size(1) # node_num = side_len**2\n","\n","    x = torch.matmul(input, self.W) # => (batch_size x nodes_num x out_features*heads)\n","    print(\"1\",x.size())\n","\n","    x = x.view(-1, N, self.heads, self.out_features) # => (batch_size x nodes_num x self.heads x out_features)\n","    print(\"2\",x.size())\n","\n","    x = x.permute(2, 0, 1, 3) # => (heads x batch_size x nodes_num x out_features)\n","    print(\"3\",x.size())\n","    # Attention Mechanism\n","    attn = []\n","    zero_vec  = -9e15 * torch.ones(N, N).to(device)\n","\n","    for i, a in zip(x, self.a):\n","      for idx, j in enumerate(i):\n","        print(idx)\n","        attn_r = torch.cat([j.view(N,1,self.out_features).expand(N,N,self.out_features).reshape(N*N, self.out_features),\n","                            j.view(1,N,self.out_features).expand(N,N,self.out_features).reshape(N*N, self.out_features)], \n","                           dim=-1).view(N,N,2*self.out_features)\n","        # attn_in = torch.cat([j.expand(-1, self.out_features*N).view(N*N, self.out_features), j.expand(N*N,1)], dim=-1).view(N,N,2*self.out_features)\n","        # => (N x N x 2*out_dim)\n","        attn_r = self.leakyrelu(torch.matmul(attn_r, a)).squeeze()\n","        # => (N x N)\n","        attn_r = torch.where(adj[idx] > 0, attn_r, zero_vec)\n","        attn_r = self.softmax(attn_r)\n","        attn_r = self.dropout(attn_r)\n","        attn.append(torch.matmul(attn_r, i)) # => (N, out_dim)\n","        del attn_r\n","                    \n","    x = torch.cat(attn, 0).reshape(self.heads, -1, N, self.out_features)\n","    # Average attention score\n","\n","    x = torch.mean(x, 0)\n","    print(\"8\", x.size())\n","    # => (batch_size x nodes_num x out_dim)\n","    if self.concat:\n","        return self.elu(x)\n","    else:\n","        return x\n","class GAT(nn.Module):\n","  def __init__(self, nfeat, nhid, nemb, dropout=0.6, batch_size=4, seq_len=24, heads=6):\n","    super(GAT, self).__init__()\n","    self.conv1 = GATConv(nfeat, nhid, heads=heads)\n","    self.conv2 = GATConv(nhid, nemb, heads=heads)\n","    self.dropout = nn.Dropout(dropout)\n","    self.elu = nn.ELU()\n","\n","  def forward(self, x, adj):\n","    # Dropout before the GAT layer is used to avoid overfitting in small datasets like Cora.\n","    # One can skip them if the dataset is sufficiently large.\n","\n","    # Transform x and adj to batch\n","    print(\"begin GAT\")\n","    node_num = x.size(-2)\n","    feature_dim = x.size(-1)\n","    batch_size = adj.size(0)\n","    seq_len = adj.size(1)\n","    adj = adj.view(batch_size*seq_len, node_num, node_num) # => (batch_size x node_num x node_num)\n","    x = x.view(-1, node_num, feature_dim).expand(batch_size*seq_len, node_num, feature_dim) # => (batch_size (or 1) x node_num x feature_dim)\n","    print(\"x size\",x.size(), \"adj size\", adj.size())\n","\n","    x = self.dropout(x)\n","\n","    x = self.elu(self.conv1(x, adj))\n","    x = self.dropout(x)\n","    x = self.conv1(x, adj)\n","    x = torch.reshape(x, (batch_size, seq_len, node_num, nemb))\n","    print(\"out size\", x.size())\n","    return x"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNcWLjT_BeQk","executionInfo":{"status":"ok","timestamp":1618367837945,"user_tz":-480,"elapsed":35607,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["# GCN for multi-view embedding\n","class GraphConvolution(nn.Module):\n","  \"\"\"\n","  Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n","  \"\"\"\n","\n","  def __init__(self, in_features, out_features, bias=True):\n","    super(GraphConvolution, self).__init__()\n","    self.in_features = in_features\n","    self.out_features = out_features\n","    self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n","    if bias:\n","        self.bias = Parameter(torch.FloatTensor(out_features))\n","    else:\n","        self.register_parameter('bias', None)\n","    self.reset_parameters()\n","\n","  def reset_parameters(self):\n","    stdv = 1. / math.sqrt(self.weight.size(1))\n","    self.weight.data.uniform_(-stdv, stdv)\n","    if self.bias is not None:\n","        self.bias.data.uniform_(-stdv, stdv)\n","\n","  def forward(self, input, adj):\n","    support = torch.matmul(input, self.weight)\n","    output = torch.matmul(adj, support)\n","\n","    if self.bias is not None:\n","        return output + self.bias\n","    else:\n","        return output\n","\n","  def __repr__(self):\n","    return self.__class__.__name__ + ' (' \\\n","            + str(self.in_features) + ' -> ' \\\n","            + str(self.out_features) + ')'\n","\n","class GCN(nn.Module):\n","  def __init__(self, nfeat, nhid, nemb, dropout, seq_len):\n","    super(GCN, self).__init__()\n","\n","    self.gc1 = GraphConvolution(nfeat, nhid)\n","    self.gc2 = GraphConvolution(nhid, nemb)\n","    self.dropout = dropout\n","\n","  def forward(self, x, adj):\n","    x = F.relu(self.gc1(x, adj))\n","    x = F.dropout(x, self.dropout, training=self.training)\n","    x = self.gc2(x, adj)\n","    return x\n","\n","# 这个就先不用了\n","# class MultiViewEmbed(nn.Module): \n","#   def __init__(self, nemb, side_len, embeds, n_view): # N views\n","#     super(MultiViewEmbed, self).__init__()\n","# #         embed = GCN(nfeat, nhid, nemb, dropout, seq_len)\n","#     self.embeds = embeds\n","#     self.fc1 = nn.Linear(n_view*nemb, nemb)\n","#     self.fc2 = nn.Linear((side_len**2)*nemb, nemb)\n","#     self.relu = nn.ReLU(True)\n","      \n","#   def forward(self, input):\n","#     x, adjs = input[0], input[1]\n","#     embeddings = [emb(x, adj) for emb, adj in zip(self.embeds, adjs)]        \n","#     embeddings = torch.cat(embeddings,dim=-1)\n","#     embeddings = self.relu(self.fc1(embeddings))\n","#     embeddings = embeddings.contiguous().view(embeddings.size(0), embeddings.size(1), -1)\n","#     embeddings = self.relu(self.fc2(embeddings))\n","#     return embeddings\n","\n","# 暂时使用这个\n","class SingleViewEmbed(nn.Module):   \n","  def __init__(self, nemb, side_len, embed):\n","    super(SingleViewEmbed, self).__init__()\n","\n","    self.embed = embed\n","    self.fc = nn.Linear((side_len**2)*nemb, nemb)\n","    self.relu = nn.ReLU(True)\n","  \n","  def forward(self, input):\n","    x, adj = input[0], input[1]\n","    embedding = self.embed(x, adj)\n","    # -> (batch_size, seq_len, nodes(side_len*side_len), emb_dim)\n","    embedding = embedding.contiguous().view(embedding.size(0), embedding.size(1), -1)\n","    # -> (batch_size, seq_len, nodes*emb_dim)\n","    embedding = self.relu(self.fc(embedding))\n","    # -> (batch_size, seq_len, emb_dim)\n","    # 这一步将向量的维度压缩的太厉害了，是否可以考虑改进一下（CNN）\n","    return embedding\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfzIw3c-ef3B","executionInfo":{"status":"ok","timestamp":1618367837945,"user_tz":-480,"elapsed":35604,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["class SingleViewEmbedCNN(nn.Module):\n","  def __init__(self, nemb, side_len, embed):\n","    super(SingleViewEmbedCNN, self).__init__()\n","\n","    self.embed = embed\n","    # 这个地方的channels维度是可以调的\n","    self.conv1 = nn.Conv2d(in_channels=nemb, out_channels=nemb, kernel_size=4, stride=2, padding=1, bias=False)        \n","    self.conv2 = nn.Conv2d(in_channels=nemb, out_channels=nemb, kernel_size=4, stride=2, padding=1, bias=False)\n","    self.conv3 = nn.Conv2d(in_channels=nemb, out_channels=nemb, kernel_size=4, stride=2, padding=1, bias=False)\n","    self.conv4 = nn.Conv2d(in_channels=nemb, out_channels=nemb, kernel_size=4, stride=1, padding=0)\n","\n","    # self.bn1 = nn.BatchNorm2d(nemb)\n","    self.bn1 = nn.BatchNorm2d(nemb)\n","    self.bn2 = nn.BatchNorm2d(nemb)\n","    self.bn3 = nn.BatchNorm2d(nemb)\n","    self.relu = nn.ReLU(True)\n","    self.sigmoid = nn.Sigmoid()\n","    self.side_len = side_len\n","  \n","  def forward(self, input):\n","    x, adj = input[0], input[1]\n","    embedding = self.embed(x, adj)\n","\n","    batch_size = embedding.size(0)\n","    seq_len = embedding.size(1)\n","\n","    # -> (batch_size, seq_len, nodes(side_len*side_len), emb_dim)\n","    embedding = embedding.contiguous().view(batch_size*seq_len, self.side_len, self.side_len, -1)\n","    embedding = embedding.permute(0, 3, 1, 2)  \n","    embedding = self.relu(self.conv1(embedding))\n","    # (batch_size*seq_len)\n","    # -> (nemb) x 16 x 16\n","    embedding = self.relu(self.bn1(self.conv2(embedding)))\n","    # -> (nemb) x 8 x 8\n","    embedding = self.relu(self.bn2(self.conv3(embedding)))\n","    # -> (nemb) x 4 x 4\n","    embedding = self.sigmoid(self.bn3(self.conv4(embedding)))\n","    # -> (nemb) x 1 x 1\n","    embedding = embedding.squeeze()\n","    # -> (batch_size*seq_len, nemb)\n","    embedding = embedding.view(batch_size, seq_len, -1)\n","    # -> (batch_size, seq_len, nemb)\n","\n","    return embedding\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"1N6R-hOLBeQk","executionInfo":{"status":"ok","timestamp":1618367837946,"user_tz":-480,"elapsed":35601,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["# Upsampling for output target\n","class UpSampling(nn.Module):\n","  def __init__(self, nz, ngf):\n","    super(UpSampling, self).__init__()\n","    \n","    self.relu = nn.ReLU(True)\n","    self.sigmoid = nn.Sigmoid()\n","    \n","    self.convt1 = nn.ConvTranspose2d(in_channels=nz, out_channels=ngf*4, kernel_size=4, stride=1, padding=0, bias=False)        \n","    self.convt2 = nn.ConvTranspose2d(in_channels=ngf*4, out_channels=ngf*2, kernel_size=4, stride=2, padding=1, bias=False)\n","    self.convt3 = nn.ConvTranspose2d(in_channels=ngf*2, out_channels=ngf, kernel_size=4, stride=2, padding=1, bias=False)\n","    self.convt4 = nn.ConvTranspose2d(in_channels=ngf, out_channels=1, kernel_size=4, stride=2, padding=1)\n","\n","    self.bn1 = nn.BatchNorm2d(ngf*4)\n","    self.bn2 = nn.BatchNorm2d(ngf*2)\n","    self.bn3 = nn.BatchNorm2d(ngf)\n","  \n","  def forward(self, input):\n","    x = input.contiguous().view(input.size(0)*input.size(1), -1, 1, 1)\n","    # input nz\n","    x = self.relu(self.bn1(self.convt1(x)))\n","    # -> (ngf x 4) x 4 x 4\n","    x = self.relu(self.bn2(self.convt2(x)))\n","    # -> (ngf x 2) x 8 x 8\n","    x = self.relu(self.bn3(self.convt3(x)))\n","    # -> (ngf) x 16 x 16\n","    x = self.sigmoid(self.convt4(x))\n","    # -> (1) x 32 x 32\n","    return x\n","\n","# Output => embedding\n","# 这部分讲道理也是应该要用卷积来做的\n","class OutputEmbed(nn.Module):\n","  def __init__(self,in_dim,hidden,out_dim):\n","    super(OutputEmbed, self).__init__()\n","    \n","    self.relu = nn.ReLU(True)\n","    \n","    self.fc1 = nn.Linear(in_dim, hidden)\n","    self.fc2 = nn.Linear(hidden, out_dim)\n","\n","  def forward(self, x):\n","    x=x.contiguous().view(x.size(0), x.size(1), -1)\n","    return self.fc2(self.relu(self.fc1(x)))\n","      "],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"1eWu_Inr8F8D","executionInfo":{"status":"ok","timestamp":1618367837946,"user_tz":-480,"elapsed":35598,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["class OutputEmbedCNN(nn.Module):\n","  def __init__(self,ngf):\n","    super(OutputEmbedCNN, self).__init__()\n","    \n","    self.conv1 = nn.Conv2d(in_channels=1, out_channels=ngf//2, kernel_size=4, stride=2, padding=1, bias=False)        \n","    self.conv2 = nn.Conv2d(in_channels=ngf//2, out_channels=ngf, kernel_size=4, stride=2, padding=1, bias=False)\n","    self.conv3 = nn.Conv2d(in_channels=ngf, out_channels=ngf*2, kernel_size=4, stride=2, padding=1, bias=False)\n","    self.conv4 = nn.Conv2d(in_channels=ngf*2, out_channels=ngf*4, kernel_size=4, stride=1, padding=0)\n","\n","    self.bn1 = nn.BatchNorm2d(ngf)\n","    self.bn2 = nn.BatchNorm2d(ngf*2)\n","    self.bn3 = nn.BatchNorm2d(ngf*4)\n","\n","    self.relu = nn.ReLU(True)\n","    self.sigmoid = nn.Sigmoid()\n","\n","\n","  def forward(self, x):\n","    batch_size = x.size(0)\n","    seq_len = x.size(1)\n","    x = x.contiguous().view(x.size(0) * x.size(1), 1, side_len, side_len)\n","    x = self.relu(self.conv1(x))\n","    # # -> (ngf / 2) x 16 x 16\n","    x = self.relu(self.bn1(self.conv2(x)))\n","    # # -> (ngf) x 8 x 8\n","    x = self.relu(self.bn2(self.conv3(x)))\n","    # # -> (ngf x 2) x 4 x 4\n","    x = self.sigmoid(self.bn3(self.conv4(x)))\n","    # # -> (ngf x 4) x 1 x 1\n","    x = x.squeeze()\n","    x = x.contiguous().view(batch_size, seq_len, -1)\n","    return x    \n","        \n","        \n","        # # -> (ngf / 2) x 16 x 16\n","    # x = self.relu(self.bn1(self.conv2(x)))\n","    # -> (ngf) x 8 x 8\n","    # x = self.relu(self.bn2(self.conv3(x)))\n","    # -> (ngf x 2) x 4 x 4\n","   "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQe5e3JQBeQl","executionInfo":{"status":"ok","timestamp":1618367837947,"user_tz":-480,"elapsed":35595,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["# Transformer for learning time dependent\n","class EncoderDecoder(nn.Module):\n","  \"\"\"\n","  A standard Encoder-Decoder architecture. Base for this and many \n","  other models.\n","  \"\"\"\n","  def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","    super(EncoderDecoder, self).__init__()\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.src_embed = src_embed\n","    self.tgt_embed = tgt_embed\n","    self.generator = generator\n","      \n","  def forward(self, src, tgt, tgt_mask, src_mask=None,):\n","    \"Take in and process masked src and target sequences.\"\n","    return self.decode(self.encode(src, src_mask), src_mask,\n","                        tgt, tgt_mask)\n","  \n","  def encode(self, src, src_mask):\n","    x = self.src_embed(src)\n","    return self.encoder(x, src_mask)\n","  \n","  def decode(self, memory, src_mask, tgt, tgt_mask):\n","    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n","\n","\n","class Encoder(nn.Module):\n","  \"Core encoder is a stack of N layers\"\n","  def __init__(self, layer, N):\n","    super(Encoder, self).__init__()\n","    self.layers = clones(layer, N)\n","    self.norm = LayerNorm(layer.size)\n","      \n","  def forward(self, x, mask):\n","    \"Pass the input (and mask) through each layer in turn.\"\n","    for layer in self.layers:\n","        x = layer(x, mask)\n","    return self.norm(x)\n","    \n","# 残差连接\n","class SublayerConnection(nn.Module):\n","  \"\"\"\n","  A residual connection followed by a layer norm.\n","  Note for code simplicity the norm is first as opposed to last.\n","  \"\"\"\n","  def __init__(self, size, dropout):\n","    super(SublayerConnection, self).__init__()\n","    self.norm = LayerNorm(size)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, sublayer):\n","    \"Apply residual connection to any sublayer with the same size.\"\n","    return x + self.dropout(sublayer(self.norm(x)))\n","\n","class EncoderLayer(nn.Module):\n","  \"Encoder is made up of self-attn and feed forward (defined below)\"\n","  def __init__(self, size, self_attn, feed_forward, dropout):\n","    super(EncoderLayer, self).__init__()\n","    self.self_attn = self_attn\n","    self.feed_forward = feed_forward\n","    self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","    self.size = size\n","\n","  def forward(self, x, mask):\n","    # \"Follow Figure 1 (left) for connections.\"\n","    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","    return self.sublayer[1](x, self.feed_forward)\n","    \n","class Decoder(nn.Module):\n","  \"Generic N layer decoder with masking.\"\n","  def __init__(self, layer, N):\n","    super(Decoder, self).__init__()\n","    self.layers = clones(layer, N)\n","    self.norm = LayerNorm(layer.size)\n","      \n","  def forward(self, x, memory, src_mask, tgt_mask):\n","    for layer in self.layers:\n","        x = layer(x, memory, src_mask, tgt_mask)\n","    return self.norm(x)\n","\n","class DecoderLayer(nn.Module):\n","  \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n","  def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","    super(DecoderLayer, self).__init__()\n","    self.size = size\n","    self.self_attn = self_attn\n","    self.src_attn = src_attn\n","    self.feed_forward = feed_forward\n","    self.sublayer = clones(SublayerConnection(size, dropout), 3)\n","\n","  def forward(self, x, memory, src_mask, tgt_mask):\n","    \"Follow Figure 1 (right) for connections.\"\n","    m = memory\n","    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","    return self.sublayer[2](x, self.feed_forward)\n","\n","def attention(query, key, value, mask=None, dropout=None):\n","  \"Compute Scaled Dot Product Attention'\"\n","  d_k = query.size(-1)\n","  scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","                              / math.sqrt(d_k)\n","  if mask is not None:\n","    scores = scores.masked_fill(mask == 0, -1e9)\n","  p_attn = F.softmax(scores, dim = -1)\n","  if dropout is not None:\n","    p_attn = dropout(p_attn)\n","  return torch.matmul(p_attn, value), p_attn\n","\n","class MultiHeadedAttention(nn.Module):\n","  def __init__(self, h, d_model, dropout=0.1):\n","    \"Take in model size and number of heads.\"\n","    super(MultiHeadedAttention, self).__init__()\n","    assert d_model % h == 0\n","    # We assume d_v always equals d_k\n","    self.d_k = d_model // h\n","    self.h = h\n","    self.linears = clones(nn.Linear(d_model, d_model), 4)\n","    self.attn = None\n","    self.dropout = nn.Dropout(p=dropout)\n","      \n","  def forward(self, query, key, value, mask=None):\n","    \"Implements Figure 2\"\n","    if mask is not None:\n","        # Same mask applied to all h heads.\n","        mask = mask.unsqueeze(1)\n","    nbatches = query.size(0)\n","    \n","    # 1) Do all the linear projections in batch from d_model => h x d_k \n","    query, key, value = \\\n","        [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","          for l, x in zip(self.linears, (query, key, value))]\n","    \n","    # 2) Apply attention on all the projected vectors in batch. \n","    x, self.attn = attention(query, key, value, mask=mask, \n","                              dropout=self.dropout)\n","    \n","    # 3) \"Concat\" using a view and apply a final linear. \n","    x = x.transpose(1, 2).contiguous() \\\n","          .view(nbatches, -1, self.h * self.d_k)\n","    return self.linears[-1](x)\n","    \n","    \n","# Connect each layer in encoder or decoder\n","class PositionwiseFeedForward(nn.Module):\n","  \"Implements FFN equation.\"\n","  def __init__(self, d_model, d_ff, dropout=0.1):\n","    super(PositionwiseFeedForward, self).__init__()\n","    self.w_1 = nn.Linear(d_model, d_ff)\n","    self.w_2 = nn.Linear(d_ff, d_model)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x):\n","    return self.w_2(self.dropout(F.relu(self.w_1(x))))\n","    \n","# Encoding the time position and add to embedding\n","class PositionalEncoding(nn.Module):\n","  \"Implement the PE function.\"\n","  def __init__(self, d_model, dropout, max_len=5000):\n","    super(PositionalEncoding, self).__init__()\n","    self.dropout = nn.Dropout(p=dropout)\n","    \n","    # Compute the positional encodings once in log space.\n","    pe = torch.zeros(max_len, d_model)\n","    position = torch.arange(0, max_len).unsqueeze(1)\n","    div_term = torch.exp(torch.arange(0, d_model, 2) *\n","                          -(math.log(10000.0) / d_model))\n","    pe[:, 0::2] = torch.sin(position * div_term)\n","    pe[:, 1::2] = torch.cos(position * div_term)\n","    pe = pe.unsqueeze(0)\n","    self.register_buffer('pe', pe)\n","      \n","  def forward(self, x):\n","    x = x + Variable(self.pe[:, :x.size(1)], \n","                      requires_grad=False)\n","    return self.dropout(x)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Op8ARveUBeQp","executionInfo":{"status":"ok","timestamp":1618367837948,"user_tz":-480,"elapsed":35592,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["def make_model(n_features=32, side_len=16, hidden=512, n_blocks=2, seq_len=24, \\\n","               d_model=128, d_ff=512, ngf=32, heads=8, dropout=0.1, views=1):\n","  \"\"\"\n","  Construct a model from hyperparameters.\n","  \n","  Parameters:\n","    seq_len - (int) the length of sequence in the task (including start symbol)\n","    side_len - (int) The city is divided into grids with sides side_len\n","    n_features - (int) the dim of node features\n","    hidden - (int) GCN hidden dim\n","    d_model - (int) embedding dim at each time position\n","    d_ff - (int) hidden dim of position-wise feed forward network\n","    n_blocks - (int) number of block repeats in Encode and Decode\n","    heads - (int) number of attention heads\n","    dropout - (float) dropout rate\n","    \n","  Returns:\n","    Full model.\n","\n","  Raises:\n","    None, todo\n","  \"\"\"\n","  c = copy.deepcopy\n","  attn = MultiHeadedAttention(heads, d_model)\n","  ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","  position = PositionalEncoding(d_model, dropout)\n","\n","  if views > 1:\n","    embeds = nn.ModuleList([GCN(n_features, hidden, d_model, dropout, seq_len) for _ in range(views)])\n","\n","    model = EncoderDecoder(\n","      Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), n_blocks),\n","      Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), n_blocks),\n","      nn.Sequential(MultiViewEmbed(d_model, side_len, embeds, views), c(position)),\n","      nn.Sequential(OutputEmbed(side_len**2,hidden, d_model),c(position)),\n","      UpSampling(d_model, ngf)\n","    ) \n","  \n","  else:\n","    embed = GCN(n_features, hidden, d_model, dropout, seq_len)\n","\n","    model = EncoderDecoder(\n","      Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), n_blocks),\n","      Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), n_blocks),\n","      nn.Sequential(SingleViewEmbed(d_model, side_len, embed), c(position)),\n","      # nn.Sequential(SingleViewEmbedCNN(d_model, side_len, embed), c(position)),\n","      # nn.Sequential(OutputEmbed(side_len**2,hidden, d_model),c(position)),\n","      nn.Sequential(OutputEmbedCNN(ngf),c(position)),\n","      UpSampling(d_model, ngf)\n","  ) \n","  # todo GCN params\n","  \n","  # This was important from their code. \n","  # Initialize parameters with Glorot / fan_avg.\n","  \n","  for p in model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","          \n","  # 上面的代码会修改GCN的初始化，后续看看能不能修改\n","  # 先不管\n","  return model.to(device)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"huCSak-tBeQs","executionInfo":{"status":"ok","timestamp":1618367837948,"user_tz":-480,"elapsed":35589,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["def loss_fn(X,Y):\n","    \n","  Xs = X.split(1, dim=0)\n","  Ys = Y.split(1, dim=0)\n","  F_norm = 0\n","  for x, y in zip(Xs, Ys):\n","    F_norm += LA.norm(x-y)\n","    # F_norm += (x-y).sum()\n","  # return F_norm \n","  return F_norm / len(Xs)\n","\n","mse = nn.MSELoss()\n","\n","def RMSE(x, y):\n","  return torch.sqrt(mse(x, y))\n","\n","def MAPE(x, y):\n","  return torch.abs((x - y) / y).mean() * 100\n","    \n","# class LossCompute:\n","#     \"A simple loss compute and train function.\"\n","#     def __init__(self, generator, criterion, opt=None):\n","#         self.generator = generator\n","#         self.criterion = criterion\n","#         self.opt = opt\n","        \n","#     def __call__(self, x, y, norm=24*16):\n","#         x = self.generator(x)\n","# #         loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n","# #                               y.contiguous().view(-1)) / norm\n","#         loss = self.criterion(x, y) / norm\n","#         loss.backward()\n","#         if self.opt is not None:\n","#             self.opt.step()\n","#             self.opt.optimizer.zero_grad()\n","#         return loss.item() * norm"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"O_3SPWBLBeQs","executionInfo":{"status":"ok","timestamp":1618367837948,"user_tz":-480,"elapsed":35586,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["# 修改multi-view\n","class Batch:\n","  \"Object for holding a batch of data with mask during training.\"\n","  def __init__(self, srcs, trg=None):\n","    self.srcs = srcs\n","    self.src_mask = None\n","    if trg is not None:\n","        self.trg = trg[:, :-1]\n","        self.trg_y = trg[:, 1:]\n","        self.trg_mask = \\\n","            self.make_std_mask(self.trg)\n","        self.ntokens = int(self.trg_y.size(0) * self.trg_y.size(1))\n","  \n","  @staticmethod\n","  def make_std_mask(tgt):\n","    \"Create a mask to hide future words.\"\n","    batch, seq_len = tgt.size(0),tgt.size(1)\n","    tgt_mask = torch.ones(batch, seq_len).unsqueeze(-2)\n","    tgt_mask = tgt_mask == 1\n","    tgt_mask = tgt_mask & Variable(\n","        subsequent_mask(seq_len).type_as(tgt_mask.data))\n","    return tgt_mask\n","\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"FXmAeiSTBeQt","executionInfo":{"status":"ok","timestamp":1618367837949,"user_tz":-480,"elapsed":35583,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":["def batch_gen(X, dataloader, start_symbol=-1e9):\n","  \"\"\"\n","  Generate random data for a urban status prediction task.\n","\n","  Parameters:\n","    batch - (int) the size of batch\n","    nbatches - (int) the num of total batch\n","    seq_len - (int) the length of sequence in the task (including start symbol)\n","    side_len - (int) The city is divided into grids with sides side_len\n","    n_features - (int) the dim of node features\n","    start_symbol - (float) represents the beginning of a sequence\n","\n","  Returns:\n","    A data iterator, create one batch each time\n","\n","  Raises:\n","    None, todo\n","  \"\"\"\n","  X = Variable(torch.from_numpy(X), requires_grad=False).float()\n","  for i_batch, (corr, speed) in enumerate(dataloader):\n","    corr[:, 0] = start_symbol\n","    speed[:, 0] = start_symbol\n","    \n","    src = Variable(corr, requires_grad=False).float()\n","    tgt = Variable(speed, requires_grad=False).float()\n","    yield copy.deepcopy(X), Batch(src, tgt)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"gswf9JbXBeQt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618367838299,"user_tz":-480,"elapsed":35929,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}},"outputId":"60cebe24-c0eb-4a4d-c153-daab063dd9b0"},"source":["# 这里是必须要修改的\n","# model = MultiViewEmbed(nfeat=32, nhid=512, nemb=128, dropout=0.1, seq_len=24, side_len=16, N=2)\n","model = make_model(n_features=148, side_len=32, n_blocks=n_blocks, heads=heads, d_model=d_model, hidden=2*d_model,ngf=ngf)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9)\n","\n","print(model)\n","def run_epoch(epoch):\n","  start = time.time()\n","  data_iter = batch_gen(nodes_features, train_dataloader)\n","  test_iter = batch_gen(nodes_features, test_dataloader)\n","  train_losses = []\n","  for i, (x,batch) in enumerate(data_iter):\n","    model.train()\n","    optimizer.zero_grad()\n","    x = x.to(device)\n","    # print(\"batch \",i)\n","    adjs = batch.srcs.to(device)\n","    src = (x, adjs)\n","    out = model(src=src, tgt=batch.trg.to(device), tgt_mask=batch.trg_mask.to(device))\n","    y_pred = model.generator(out)\n","    y_pred = y_pred.view(batch_size, seq_len-1, side_len, side_len)\n","    # print(y_pred.size(), batch.trg_y.size())\n","    loss = loss_fn(y_pred, batch.trg_y.to(device))\n","    train_losses.append(loss.item())\n","    loss.backward()\n","    optimizer.step()\n","  train_end = time.time()\n","  print(\"train loss \", np.mean(train_losses), \"time \", train_end - start, \"s\")\n","  # model evaluation\n","  test_losses = []\n","  rmse_losses = []\n","  mape_losses = []\n","  for x, batch in test_iter:\n","    model.eval()\n","    x = x.to(device)\n","    adjs = batch.srcs.to(device)\n","    src = (x, adjs)\n","    out = model(src=src, tgt=batch.trg.to(device), tgt_mask=batch.trg_mask.to(device))\n","    y_pred = model.generator(out)\n","    y_pred = y_pred.view(1, seq_len-1, side_len, side_len)\n","    loss = loss_fn(y_pred, batch.trg_y.to(device))\n","    rmse_loss = RMSE(y_pred, batch.trg_y.to(device))\n","    mape_loss = MAPE(y_pred, batch.trg_y.to(device))\n","    test_losses.append(loss.item())\n","    rmse_losses.append(rmse_loss.item())\n","    mape_losses.append(mape_loss.item())\n","  test_end = time.time()\n","  print(\"test loss \", np.mean(test_losses), \"time \", test_end - train_end, \"s\")\n","  print(\"RMSE:\", rmse_loss.item(), \"MAPE:\", mape_loss.item())\n","\n","  if rmse_loss.item() < 0.12 and mape_loss.item() < 16.5:\n","    torch.save(model.state_dict(), model_out_path + str(epoch) + '.pkl')\n","  # visualization\n","  y = batch.trg_y[0,0,:,:].squeeze().cpu() * 70\n","  y_hat = y_pred[0,0,:,:].squeeze().detach().cpu() * 70\n","  plt.figure()\n","  fig, ax =plt.subplots(1,2,figsize=(16,6))\n","  sns.heatmap(y_hat, ax=ax[0], vmin=0, vmax=70)\n","  sns.heatmap(y, ax=ax[1], vmin=0, vmax=70)\n","  plt.show()\n","\n","def main():\n","  for i in range(EPOCHS):\n","    print(\"epoch \",i)\n","    run_epoch(i)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["EncoderDecoder(\n","  (encoder): Encoder(\n","    (layers): ModuleList(\n","      (0): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=64, out_features=64, bias=True)\n","            (1): Linear(in_features=64, out_features=64, bias=True)\n","            (2): Linear(in_features=64, out_features=64, bias=True)\n","            (3): Linear(in_features=64, out_features=64, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=64, out_features=512, bias=True)\n","          (w_2): Linear(in_features=512, out_features=64, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=64, out_features=64, bias=True)\n","            (1): Linear(in_features=64, out_features=64, bias=True)\n","            (2): Linear(in_features=64, out_features=64, bias=True)\n","            (3): Linear(in_features=64, out_features=64, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=64, out_features=512, bias=True)\n","          (w_2): Linear(in_features=512, out_features=64, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): EncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=64, out_features=64, bias=True)\n","            (1): Linear(in_features=64, out_features=64, bias=True)\n","            (2): Linear(in_features=64, out_features=64, bias=True)\n","            (3): Linear(in_features=64, out_features=64, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=64, out_features=512, bias=True)\n","          (w_2): Linear(in_features=512, out_features=64, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=64, out_features=64, bias=True)\n","            (1): Linear(in_features=64, out_features=64, bias=True)\n","            (2): Linear(in_features=64, out_features=64, bias=True)\n","            (3): Linear(in_features=64, out_features=64, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=64, out_features=64, bias=True)\n","            (1): Linear(in_features=64, out_features=64, bias=True)\n","            (2): Linear(in_features=64, out_features=64, bias=True)\n","            (3): Linear(in_features=64, out_features=64, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=64, out_features=512, bias=True)\n","          (w_2): Linear(in_features=512, out_features=64, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=64, out_features=64, bias=True)\n","            (1): Linear(in_features=64, out_features=64, bias=True)\n","            (2): Linear(in_features=64, out_features=64, bias=True)\n","            (3): Linear(in_features=64, out_features=64, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=64, out_features=64, bias=True)\n","            (1): Linear(in_features=64, out_features=64, bias=True)\n","            (2): Linear(in_features=64, out_features=64, bias=True)\n","            (3): Linear(in_features=64, out_features=64, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=64, out_features=512, bias=True)\n","          (w_2): Linear(in_features=512, out_features=64, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): DecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=64, out_features=64, bias=True)\n","            (1): Linear(in_features=64, out_features=64, bias=True)\n","            (2): Linear(in_features=64, out_features=64, bias=True)\n","            (3): Linear(in_features=64, out_features=64, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (src_attn): MultiHeadedAttention(\n","          (linears): ModuleList(\n","            (0): Linear(in_features=64, out_features=64, bias=True)\n","            (1): Linear(in_features=64, out_features=64, bias=True)\n","            (2): Linear(in_features=64, out_features=64, bias=True)\n","            (3): Linear(in_features=64, out_features=64, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=64, out_features=512, bias=True)\n","          (w_2): Linear(in_features=512, out_features=64, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (sublayer): ModuleList(\n","          (0): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): SublayerConnection(\n","            (norm): LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm()\n","  )\n","  (src_embed): Sequential(\n","    (0): SingleViewEmbed(\n","      (embed): GCN(\n","        (gc1): GraphConvolution (148 -> 128)\n","        (gc2): GraphConvolution (128 -> 64)\n","      )\n","      (fc): Linear(in_features=65536, out_features=64, bias=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (tgt_embed): Sequential(\n","    (0): OutputEmbedCNN(\n","      (conv1): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (conv2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (conv3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (conv4): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))\n","      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (sigmoid): Sigmoid()\n","    )\n","    (1): PositionalEncoding(\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (generator): UpSampling(\n","    (relu): ReLU(inplace=True)\n","    (sigmoid): Sigmoid()\n","    (convt1): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (convt2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (convt3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (convt4): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aC6phnUaBeQu","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1BfyQBTm_Ieg1q4gP0fFxS7PeO8sfxsKd"},"executionInfo":{"status":"ok","timestamp":1618372359760,"user_tz":-480,"elapsed":4557384,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}},"outputId":"fe8bfcc9-cdf0-4c9d-dce8-fcd01a6aa505"},"source":["main()"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"G56lux0iIJVA","executionInfo":{"status":"ok","timestamp":1618372359761,"user_tz":-480,"elapsed":4557378,"user":{"displayName":"Damian Gao","photoUrl":"","userId":"14670160567213968619"}}},"source":[""],"execution_count":18,"outputs":[]}]}